import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
import os

# Generate a synthetic dataset
def generate_data(n_samples=10000, n_features=20, n_classes=2, test_size=0.2, random_state=42):
    """
    Generates synthetic classification data.
    Splits the data into training and testing sets.

    Args:
        n_samples (int): Number of samples to generate.
        n_features (int): Number of features for each sample.
        n_classes (int): Number of output classes.
        test_size (float): Proportion of the dataset to use as the test set.
        random_state (int): Seed for reproducibility.

    Returns:
        tuple: Training and testing data (X_train, X_test, y_train, y_test).
    """
    X, y = make_classification(
        n_samples=n_samples, n_features=n_features, n_classes=n_classes, random_state=random_state
    )
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)
    return X_train, X_test, y_train, y_test

# Define and compile a simple neural network model
def build_model(input_shape, learning_rate=0.001):
    """
    Builds and compiles a simple neural network model.

    Args:
        input_shape (tuple): Shape of the input data.
        learning_rate (float): Learning rate for the optimizer.

    Returns:
        keras.Model: Compiled neural network model.
    """
    model = models.Sequential([
        layers.Dense(128, activation='relu', input_shape=input_shape),
        layers.Dropout(0.3),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(32, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Use tf.data for faster data loading and preprocessing
def prepare_data(X, y, batch_size=32):
    """
    Prepares the data using TensorFlow's tf.data API.

    Args:
        X (np.ndarray): Input features.
        y (np.ndarray): Labels.
        batch_size (int): Batch size for the dataset.

    Returns:
        tf.data.Dataset: Prepared TensorFlow dataset.
    """
    dataset = tf.data.Dataset.from_tensor_slices((X, y))
    dataset = dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)
    return dataset

# Save the trained model
def save_model(model, model_path="model.h5"):
    """
    Saves the trained model to the specified path.

    Args:
        model (keras.Model): Trained model to save.
        model_path (str): Path to save the model.
    """
    model.save(model_path)
    print(f"Model saved to {model_path}")

# Load a saved model
def load_model(model_path="model.h5"):
    """
    Loads a model from the specified path.

    Args:
        model_path (str): Path to the saved model.

    Returns:
        keras.Model: Loaded model.
    """
    if os.path.exists(model_path):
        print(f"Loading model from {model_path}")
        return tf.keras.models.load_model(model_path)
    else:
        print(f"No model found at {model_path}. Returning None.")
        return None

# Train the model
def train_model(model, train_dataset, test_dataset, epochs=10):
    """
    Trains the model and evaluates its performance.

    Args:
        model (keras.Model): Model to train.
        train_dataset (tf.data.Dataset): Training dataset.
        test_dataset (tf.data.Dataset): Testing dataset.
        epochs (int): Number of epochs to train.

    Returns:
        keras.callbacks.History: Training history.
    """
    history = model.fit(train_dataset, epochs=epochs, validation_data=test_dataset)
    test_loss, test_acc = model.evaluate(test_dataset)
    print(f"Final Test Accuracy: {test_acc:.2f}")
    return history

# Plot training history
def plot_training_history(history):
    """
    Plots the training and validation accuracy and loss.

    Args:
        history (keras.callbacks.History): Training history.
    """
    import matplotlib.pyplot as plt

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(len(acc))

    plt.figure(figsize=(12, 5))

    # Accuracy plot
    plt.subplot(1, 2, 1)
    plt.plot(epochs, acc, label='Training Accuracy')
    plt.plot(epochs, val_acc, label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()

    # Loss plot
    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, label='Training Loss')
    plt.plot(epochs, val_loss, label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.legend()

    plt.show()

# Main function to run the training
def main():
    # Generate and preprocess data
    X_train, X_test, y_train, y_test = generate_data()
    train_dataset = prepare_data(X_train, y_train)
    test_dataset = prepare_data(X_test, y_test)

    # Check if a saved model exists
    model = load_model()
    if model is None:
        # Build and train a new model if no saved model is found
        model = build_model((X_train.shape[1],))
        history = train_model(model, train_dataset, test_dataset, epochs=10)

        # Save the trained model
        save_model(model)

        # Plot the training history
        plot_training_history(history)
    else:
        # Evaluate the loaded model
        test_loss, test_acc = model.evaluate(test_dataset)
        print(f"Loaded Model Test Accuracy: {test_acc:.2f}")

if __name__ == "__main__":
    main()
